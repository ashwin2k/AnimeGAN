{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,

   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from math import log2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,

   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,

   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 7,

   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernal_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=kernal_size, stride=stride, padding=padding)\n",
    "        self.scale = (gain/(in_channels*kernal_size**2))**0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x*self.scale)+self.bias.view(1, self.bias.shape[0], 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,

   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x/torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 9,

   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pixelnorm=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pn = pixelnorm\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 10,

   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0, 2),\n",
    "            WSConv2d(in_channels, in_channels,\n",
    "                     kernal_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "        self.rgb = WSConv2d(in_channels, img_channels,\n",
    "                            kernal_size=1, stride=1, padding=0)\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([\n",
    "            self.rgb])\n",
    "\n",
    "        for i in range(len(factors)-1):\n",
    "            conv_in_c = int(in_channels*factors[i])\n",
    "            conv_out_c = int(in_channels*factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernal_size=1, stride=1, padding=0))\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha*generated+(1-alpha)*upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.rgb(out)\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "        final_upscaled = self.rgb_layers[steps-1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,

   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors)-1, 0, -1):\n",
    "            conv_in_c = int(in_channels*factors[i])\n",
    "            conv_out_c = int(in_channels*factors[i-1])\n",
    "            self.prog_blocks.append(\n",
    "                ConvBlock(conv_in_c, conv_out_c, pixelnorm=False))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in_c, kernal_size=1, stride=1, padding=0))\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernal_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels+1, in_channels,\n",
    "                     kernal_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels,\n",
    "                     kernal_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernal_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscale, out):\n",
    "        return alpha*out+(1-alpha)*downscale\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_stat = torch.std(x, dim=0).mean().repeat(\n",
    "            x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_stat], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step+1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step+1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK 4\n",
      "OK 8\n",
      "OK 16\n",
      "OK 32\n",
      "OK 64\n",
      "OK 128\n",
      "OK 256\n",
      "OK 512\n",
      "OK 1024\n"
     ]
    }
   ],
   "source": [
    "Z_DIM = 50\n",
    "IN_CHANNELS = 256\n",
    "gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "disc = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "\n",
    "for resolution in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    num_steps = int(log2(resolution/4))\n",
    "    x = torch.randn((1, Z_DIM, 1, 1))\n",
    "    z = gen(x, 0.5, steps=num_steps)\n",
    "    assert z.shape == (1, 3, resolution, resolution)\n",
    "    out = disc(z, alpha=0.5, steps=num_steps)\n",
    "    assert out.shape == (1, 1)\n",
    "    print(\"OK\", resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,

   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmarks = True\n",
    "INIT_IMG_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 512\n",
    "IN_CHANNELS = 512\n",
    "DISC_ITERATIONS = 1\n",
    "LAMBDA_GP = 10\n",
    "PROGAN_EPOCHS = [10]*len(BATCH_SIZE)\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n",
    "NUM_WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME:  20230205-094156\n"

     ]
    }
   ],
   "source": [
    "print(\"TIME: \", now.strftime(\"%Y%m%d-%H%M%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,

   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTensorBoard(writer, loss_critic, loss_gen, real, fake, tensorboard_step):\n",
    "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # take out (up to) 8 examples to plot\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = BATCH_SIZE[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=\"./datasets\", transform=transform)\n",

    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset\n",
    "\n",
    "def trainFunc(disc,gen,loader,dataset,step,alpha,opt_disc,opt_gen,tb_step,writer,scaler_gen,scaler_disc):\n",
    "    loop=tqdm(loader,leave=True)\n",
    "    for batch_indx,(real,_) in enumerate(loop):\n",
    "        real=real.to(DEVICE)\n",
    "        cur_batch_size=real.shape[0]\n",
    "\n",
    "        noise=torch.randn(cur_batch_size,Z_DIM,1,1).to(DEVICE)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake=gen(noise,alpha,step)\n",
    "            critic_real=disc(real,alpha,step)\n",
    "            critic_fake=disc(fake.detach(),alpha,step)\n",
    "            gp=gradient_penalty(disc,real,fake,alpha,step,DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real)-torch.mean(critic_fake))\n",
    "                + LAMBDA_GP*gp\n",
    "                + (0.001 * torch.mean(critic_real**2))\n",
    "            )\n",
    "        opt_disc.zero_grad()\n",
    "        scaler_disc.scale(loss_critic).backward()\n",
    "        scaler_disc.step(opt_disc)\n",
    "        scaler_disc.update()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake=disc(fake,alpha,step)\n",
    "            loss_gen=-torch.mean(gen_fake)\n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        alpha+=cur_batch_size/(len(dataset)*PROGAN_EPOCHS[step]*0.5)\n",
    "        alpha=min(alpha,1)\n",
    "\n",
    "        if batch_indx%500 ==0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes=gen(FIXED_NOISE,alpha,step)*0.5 + 0.5\n",
    "            plotTensorBoard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tb_step\n",
    "            )\n",
    "            tb_step+=1\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tb_step, alpha\n",
    "\n",
    "def train_wrapper():\n",
    "    gen=Generator(Z_DIM,IN_CHANNELS,CHANNELS_IMG).to(DEVICE)\n",
    "    disc=Discriminator(Z_DIM,IN_CHANNELS,CHANNELS_IMG).to(DEVICE)\n",
    "\n",
    "    opt_gen=optim.Adam(gen.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "    opt_disc=optim.Adam(disc.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "\n",
    "    scaler_disc=torch.cuda.amp.GradScaler()\n",
    "    scaler_gen=torch.cuda.amp.GradScaler()\n",
    "\n",
    "    writer=SummaryWriter(f'logs/progan/'+ now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    \n",
    "    tb_step=0\n",
    "    step=int(log2(INIT_IMG_SIZE/4))\n",
    "    for num_epochs in PROGAN_EPOCHS[step:]:\n",
    "        alpha=1e-5\n",
    "        loader,dataset = get_loader(4*2**step)\n",
    "        print(\"IMAGE SIZE\",4*2**step)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Range\",4*2**step) \n",
    "            tb_step,alpha=trainFunc(\n",
    "                disc,gen,loader,dataset,step,alpha,opt_disc,opt_gen,tb_step,writer,scaler_gen,scaler_disc\n",
    "            )\n",
    "        torch.save(gen,'./gen_'+str(step)+'.pt')\n",
    "        torch.save(disc,'./disc_'+str(step)+'.pt')\n",

    "        step += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [

    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE SIZE 4\n",
      "Range 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987/1987 [00:47<00:00, 41.49it/s, gp=0.00745, loss_critic=0.00799] \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987/1987 [00:48<00:00, 41.07it/s, gp=0.00899, loss_critic=0.0298] \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987/1987 [00:47<00:00, 42.14it/s, gp=0.00261, loss_critic=-.0746]  \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1744/1987 [00:40<00:05, 45.14it/s, gp=0.00629, loss_critic=0.217]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1987/1987 [00:46<00:00, 42.43it/s, gp=0.0132, loss_critic=0.0596]   \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE SIZE 8\n",
      "Range 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 846/1987 [00:33<00:46, 24.62it/s, gp=0.00864, loss_critic=0.0877]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1987/1987 [01:14<00:00, 26.54it/s, gp=0.00265, loss_critic=0.15]    \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 408/1987 [00:15<01:00, 26.06it/s, gp=0.00207, loss_critic=-.0255]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1987/1987 [01:13<00:00, 26.95it/s, gp=0.0065, loss_critic=0.296]    \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE SIZE 16\n",
      "Range 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 551/1987 [00:32<01:25, 16.81it/s, gp=0.00347, loss_critic=-.0948] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1987/1987 [01:55<00:00, 17.21it/s, gp=0.00751, loss_critic=-.623]  \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1521/1987 [01:28<00:27, 16.73it/s, gp=0.00301, loss_critic=-.083]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1987/1987 [01:55<00:00, 17.22it/s, gp=0.00102, loss_critic=0.0654] \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1929/1987 [01:51<00:03, 17.04it/s, gp=0.00253, loss_critic=-.377]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 3973/3973 [06:58<00:00,  9.49it/s, gp=0.000965, loss_critic=0.331]\n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1030/3973 [01:48<05:10,  9.49it/s, gp=0.00153, loss_critic=0.396]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 91%|█████████ | 3596/3973 [06:19<00:39,  9.48it/s, gp=0.0215, loss_critic=2.11]    IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 56%|█████▌    | 2209/3973 [03:53<03:05,  9.51it/s, gp=0.01, loss_critic=-.515]     IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 3973/3973 [06:59<00:00,  9.47it/s, gp=0.00289, loss_critic=-.657]  \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 383/3973 [00:40<06:19,  9.46it/s, gp=0.0065, loss_critic=-.943]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 73%|███████▎  | 2908/3973 [05:07<01:51,  9.51it/s, gp=0.0106, loss_critic=-.335]    IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 3973/3973 [06:58<00:00,  9.49it/s, gp=0.00234, loss_critic=-.197]  \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1017/3973 [01:47<05:10,  9.51it/s, gp=0.00726, loss_critic=-.532] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 79%|███████▉  | 3155/3973 [15:27<03:59,  3.42it/s, gp=0.007, loss_critic=-1.97]     IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 39%|███▉      | 1553/3973 [07:36<11:54,  3.39it/s, gp=0.00597, loss_critic=-1.32]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 80%|████████  | 3195/3973 [15:43<03:47,  3.42it/s, gp=0.00492, loss_critic=-1.8]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 46%|████▋     | 1839/3973 [09:00<10:32,  3.37it/s, gp=0.0139, loss_critic=-.999]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 3973/3973 [19:25<00:00,  3.41it/s, gp=0.0116, loss_critic=-1.27]   \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 64\n"

     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 498/3973 [02:27<17:07,  3.38it/s, gp=0.0149, loss_critic=-.715]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 73%|███████▎  | 2914/3973 [14:20<05:15,  3.36it/s, gp=0.00863, loss_critic=-1.39]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 3973/3973 [19:40<00:00,  3.37it/s, gp=0.00515, loss_critic=0.565]   \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE SIZE 128\n",
      "Range 128\n"

     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [38:51<00:00,  1.70it/s, gp=0.00467, loss_critic=-3.12] \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 128\n"

     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [38:52<00:00,  1.70it/s, gp=0.0111, loss_critic=-1.27]    \n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 128\n"

     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1210/3973 [11:48<26:51,  1.71it/s, gp=0.017, loss_critic=-2.02]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 93%|█████████▎| 3678/3973 [35:53<02:53,  1.70it/s, gp=0.0144, loss_critic=-.763]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 89%|████████▊ | 3522/3973 [34:17<04:23,  1.71it/s, gp=0.0203, loss_critic=-.969] \n"

     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 132\u001b[0m, in \u001b[0;36mtrain_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstep) \n\u001b[0;32m--> 132\u001b[0m     tb_step,alpha\u001b[38;5;241m=\u001b[39m\u001b[43mtrainFunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtb_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler_disc\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(gen,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./gen_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(step)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(disc,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./disc_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(step)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 75\u001b[0m, in \u001b[0;36mtrainFunc\u001b[0;34m(disc, gen, loader, dataset, step, alpha, opt_disc, opt_gen, tb_step, writer, scaler_gen, scaler_disc)\u001b[0m\n\u001b[1;32m     69\u001b[0m     loss_critic \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;241m-\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmean(critic_real)\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(critic_fake))\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;241m+\u001b[39m LAMBDA_GP\u001b[38;5;241m*\u001b[39mgp\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(critic_real\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m opt_disc\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 75\u001b[0m \u001b[43mscaler_disc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_critic\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m scaler_disc\u001b[38;5;241m.\u001b[39mstep(opt_disc)\n\u001b[1;32m     77\u001b[0m scaler_disc\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "

     ]
    }
   ],
   "source": [
    "train_wrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen,'./gen_latest'+'.pt')\n",
    "torch.save(disc,'./disc_latest'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []

  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",

   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },

  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
